<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>matplotlib</title>
    <link href="/2020/05/28/matplotlib/"/>
    <url>/2020/05/28/matplotlib/</url>
    
    <content type="html"><![CDATA[<h1 id="导入matplotlib"><a href="#导入matplotlib" class="headerlink" title="导入matplotlib"></a>导入matplotlib</h1><pre><code>import matplotlib.pyplot as pltimport matplotlib as mpl</code></pre><p>&emsp;&emsp;Matplotlib是Python的绘图库，其中的pyplot包封装了很多画图的函数。<a href="https://matplotlib.org/api/pyplot_summary.html" target="_blank" rel="noopener">Matplotlib.pyplot</a>包含一系列类似 MATLAB 中绘图函数的相关函数每个pyplot函数对一幅图片(figure)做一些改动：比如创建新图片，在图片创建一个新的作图区域(plotting area)，在一个作图区域内画直线，给图添加标签(label)等。matplotlib.pyplot是有状态的，亦即它会保存当前图片和作图区域的状态，新的作图函数会作用在当前图片的状态基础之上。 </p><h1 id="打开-关闭交互模式"><a href="#打开-关闭交互模式" class="headerlink" title="打开/关闭交互模式"></a>打开/关闭交互模式</h1><pre><code>plt.ion()plt.ioff()</code></pre><p>&emsp;&emsp;matplotlib的显示模式默认为阻塞（block）模式，打开交互模式后，执行plt.show（）程序会接着往下执行，可以显示多个窗口。但在show()之前一定要使用plt.ioff()，否则图片会一闪而过。在交互模式下，plt.plot(x)或plt.imshow(x)是直接出图像，不需要plt.show()  </p><h1 id="定义画布"><a href="#定义画布" class="headerlink" title="定义画布"></a>定义画布</h1><pre><code>plt.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True)</code></pre><p>num:图像编号或名称，数字为编号 ，字符串为名称<br>figsize:指定figure的宽和高，单位为英寸；<br>dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80<br>1英寸等于2.5cm,A4纸是 21*30cm的纸张<br>facecolor:背景颜色<br>edgecolor:边框颜色<br>frameon:是否显示边框 </p><h1 id="画图并显示"><a href="#画图并显示" class="headerlink" title="画图并显示"></a>画图并显示</h1><pre><code>import matplotlib.pyplot as pltimport numpy as nppic=plt.figure(&apos;ex_1&apos;,figsize=(16,9),dpi=480,facecolor=&apos;red&apos;,edgecolor=&apos;blue&apos;)x = np.linspace(-10, 10, 500)y1=x*xy2=2*x+1plt.plot(x,y1)plt.plot(x,y2)    plt.savefig(&apos;ex_1.png&apos;)plt.show()</code></pre><p>plt.plot(自变量，因变量)将图表示出来，plt.savefig(‘文件名’)在plt.show()之前，否则保存的图是空白,plt.show()显示出来。<br><img src="/img/plt_learning/ex_1.png" srcset="/img/loading.gif" alt="">  </p><pre><code>plt.plot(x, y2, color = &apos;black&apos;, linewidth = 2.0, linestyle = &apos;--&apos;)</code></pre><p>可将y2改成虚线</p><h1 id="绘制子图"><a href="#绘制子图" class="headerlink" title="绘制子图"></a>绘制子图</h1><pre><code>plt.subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True,         subplot_kw=None, gridspec_kw=None, **fig_kw):</code></pre><p>参数：  </p><pre><code>Parameters----------nrows, ncols : int, optional, default: 1    子图网格的行列数.sharex, sharey : bool or {&apos;none&apos;, &apos;all&apos;, &apos;row&apos;, &apos;col&apos;}, default: False    Controls sharing of properties among x (`sharex`) or y (`sharey`)    axes:    - True or &apos;all&apos;: x- or y-axis will be shared among all subplots.    - False or &apos;none&apos;: each subplot x- or y-axis will be independent.    - &apos;row&apos;: each subplot row will share an x- or y-axis.    - &apos;col&apos;: each subplot column will share an x- or y-axis.    When subplots have a shared x-axis along a column, only the x tick    labels of the bottom subplot are created. Similarly, when subplots    have a shared y-axis along a row, only the y tick labels of the first    column subplot are created. To later turn other subplots&apos; ticklabels    on, use `~matplotlib.axes.Axes.tick_params`.squeeze : bool, optional, default: True    - If True, extra dimensions are squeezed out from the returned      array of `~matplotlib.axes.Axes`:      - if only one subplot is constructed (nrows=ncols=1), the        resulting single Axes object is returned as a scalar.      - for Nx1 or 1xM subplots, the returned object is a 1D numpy        object array of Axes objects.      - for NxM, subplots with N&gt;1 and M&gt;1 are returned as a 2D array.    - If False, no squeezing at all is done: the returned Axes object is      always a 2D array containing Axes instances, even if it ends up      being 1x1.num : int or str, optional, default: None    A `.pyplot.figure` keyword that sets the figure number or label.subplot_kw : dict, optional    Dict with keywords passed to the    `~matplotlib.figure.Figure.add_subplot` call used to create each    subplot.gridspec_kw : dict, optional    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`    constructor used to create the grid the subplots are placed on.**fig_kw    All additional keyword arguments are passed to the    `.pyplot.figure` call.</code></pre><p>返回：<br>参数1：figure<br>参数2：axes对象  </p><pre><code>fig, (ax1, ax2) = plt.subplot(1, 2)fig, ((ax1, ax2), (ax3, ax4)) = plt.subplot(2, 2)</code></pre><p>示例1（定义子图然后通过返回的axes操作）：</p><pre><code>import numpy as npimport matplotlib.pyplot as plt    x = np.linspace(-10, 10, 500)#划分子图fig,axes=plt.subplots(2,2,sharex=&apos;row&apos;)ax1=axes[0,0]ax2=axes[0,1]ax3=axes[1,0]ax4=axes[1,1]#作图1ax1.plot(x, x)#作图2ax2.plot(x, -x)#作图3ax3.plot(x, x ** 2)ax3.grid(color=&apos;r&apos;, linestyle=&apos;--&apos;, linewidth=1,alpha=0.3)#作图4ax4.plot(x, np.log(np.abs(x)))plt.show()</code></pre><p><img src="%5Cimg%5Cplt_learning%5Cex_subplot.png" srcset="/img/loading.gif" alt=""><br>示例2（定义画布再添加子图）：</p><pre><code>import matplotlib.pyplot as pltimport numpy as npfig=plt.figure()x=np.linspace(-10, 10, 500)ax1=fig.add_subplot(3,3,1)ax1.plot(x, x*x)ax2=fig.add_subplot(3,3,3)ax2.plot(x,np.sin(x))ax3=fig.add_subplot(3,3,5)ax3.plot(x,np.arctan(x))ax4=fig.add_subplot(3,3,9)ax4.plot(x,np.cos(x))plt.savefig(&apos;object_figure&apos;)plt.show()  </code></pre><p><img src="%5Cimg%5Cplt_learning%5Cobject_figure.png" srcset="/img/loading.gif" alt=""><br>示例3（子图嵌套） </p><pre><code>import numpy as npimport matplotlib.pyplot as plt#新建figurefig = plt.figure()# 定义数据x = np.linspace(-10,10,500)y=2*x*2*xz =2*x#新建区域ax1#figure的百分比,从figure 10%的位置开始绘制, 宽高是figure的80%left, bottom, width, height = 0.1, 0.1, 0.8, 0.8# 获得绘制的句柄ax1 = fig.add_axes([left, bottom, width, height])ax1.plot(x, y, &apos;r&apos;)ax1.set_title(&apos;4x^2&apos;)#新增区域ax2,嵌套在ax1内left, bottom, width, height = 0.35 ,0.6, 0.25, 0.25# 获得绘制的句柄ax2 = fig.add_axes([left, bottom, width, height])ax2.plot(x,z, &apos;b&apos;)ax2.set_title(&apos;2x&apos;)plt.show()</code></pre><p><img src="%5Cimg%5Cplt_learning%5Cinsert.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>development</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch入门</title>
    <link href="/2020/05/26/pytorch%E7%9A%84forward/"/>
    <url>/2020/05/26/pytorch%E7%9A%84forward/</url>
    
    <content type="html"><![CDATA[<h2 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h2><pre><code>class Module(nn.Module):def __init__(self):    super(Module, self).__init__()    # ......def forward(self, x):    # ......    return xdata = .....  #输入数据# 实例化一个对象module = Module()# 前向传播module(data)  # 而不是使用下面的# module.forward(data)   </code></pre><p>因为 python calss 中的<strong>call</strong>和<strong>init</strong>方法.使得module(data)等价与module.forward(data)</p><pre><code>#50000张训练图片  train_set = torchvision.datasets.CIFAR10(root=&apos;./data&apos;, train=True,                                    download=False,     transform=transform)train_loader = torch.utils.data.DataLoader(train_set, batch_size=36,                                      shuffle=False,     num_workers=0)</code></pre><p>数据集通过torchvision.dataset.进行设置下载，通过一个dataloader使其成为训练中可用的参数</p><pre><code> outputs = net(val_image)  # [batch, 10]predict_y = torch.max(outputs, dim=1)[1]accuracy = (predict_y == val_label).sum().item() / val_label.size(0)</code></pre><p>outputs为5000<em>10，第0维为图片序号，第一维为10个类别的数值，最大的一般视为识别的类<br>*</em>torch.max(input, dim, keepdim=False, out=None)**<br>Returns a namedtuple <strong>(values, indices)</strong> where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax).所以predict_y为最大值的索引<br>predict_y == val_label为5000个true或false组成的tensor列表用sum将它们合成一个tensor数，然后.item变为数</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>神经网络的基本内容</title>
    <link href="/2020/05/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%86%85%E5%AE%B9/"/>
    <url>/2020/05/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%86%85%E5%AE%B9/</url>
    
    <content type="html"><![CDATA[<h1 id="人工神经元"><a href="#人工神经元" class="headerlink" title="人工神经元"></a>人工神经元</h1><h1 id="bp算法"><a href="#bp算法" class="headerlink" title="bp算法"></a>bp算法</h1><h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><h2 id="1-SGD"><a href="#1-SGD" class="headerlink" title="1.SGD"></a>1.SGD</h2><h2 id="2-SGD-momentum"><a href="#2-SGD-momentum" class="headerlink" title="2.SGD+momentum"></a>2.SGD+momentum</h2><h2 id="3-Adagrad"><a href="#3-Adagrad" class="headerlink" title="3.Adagrad"></a>3.Adagrad</h2><p>学习率下降得快，可能未收敛就停止训练</p><h2 id="4-RMSProp"><a href="#4-RMSProp" class="headerlink" title="4.RMSProp"></a>4.RMSProp</h2><h2 id="5-ADAM"><a href="#5-ADAM" class="headerlink" title="5.ADAM"></a>5.ADAM</h2>]]></content>
    
    
    <categories>
      
      <category>science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>专业词汇</title>
    <link href="/2020/05/24/%E4%B8%93%E4%B8%9A%E8%AF%8D%E6%B1%87/"/>
    <url>/2020/05/24/%E4%B8%93%E4%B8%9A%E8%AF%8D%E6%B1%87/</url>
    
    <content type="html"><![CDATA[<p>fine-grained: 细粒度，类内细分<br> pose estimator 姿势估计<br>retrieval 检索；恢复；取回；拯救<br> state-of-the-art 最先进的<br>benchmarks 基准一个算法之所以被称为benchmark，是因为它的性能已经被广泛研究，人们对它性能的表现形式、测量方法都非常熟悉，因此可以作为标准方法来衡量其他方法的好坏。<br>baseline 一个算法被称为baseline，基本上表示比这个算法性能还差的基本上不能接受的，除非方法上有革命性的创新点，而且还有巨大的改进空间和超越benchmark的潜力，只是因为是发展初期而性能有限。所以baseline有一个自带的含义就是“性能起点”。<br><strong>SOTA&gt;benchmark&gt;baseline</strong><br>leverage n. 手段，影响力；杠杆作用；杠杆效率v. 利用；举债经营<br>are prone to 易于<br>partition  n. 划分，分开；[数] 分割；隔墙；隔离物vt. [数] 分割；分隔；区分<br>backbone  n. 脊骨，脊柱；支柱；骨气，毅力；书脊；（生化）聚合分子主链；（计算机）主干网<br> outliers  离群值<br>parse n. 语法分析；剖析v. 解析<br>backbone n. 脊骨，脊柱；支柱；骨气，毅力；书脊；（生化）聚合分子主链；（计算机）主干网<br>column n. 纵队，列；专栏；圆柱，柱形物<br>stripe  n. 条纹，斑纹；种类 vt. 加条纹于…<br> Cross-Entropy  n.交叉熵<br>concatenate  v. 连接，连结，使连锁 adj. 连接的，连结的，连锁的<br>spatial adj. 空间的；存在于空间的；受空间条件限制的<br>down-sampling 下采样<br>granularity  n.间隔尺寸<br>denote vt. 表示，指示<br>intuition  n. 直觉；直觉力；直觉的知识<br>non-trivial adj. 非平凡的；面对较重大<br>similarity measuring 相似性度量<br>iteratively 反复地<br>convergence n.收敛，集合<br> residual  adj. （数量）剩余的；（物质状态在成因消失后）剩余的，残留的；（实验误差）舍去的，残差的；（土壤）残余的<br>n. 剩余物，残渣；残差，剩余误差<br>stochastic gradient descent (SGD) 随机梯度下降<br>padding n.填充<br>dilation n.扩张，详述<br>nondeterministic a.不确定的<br>proposal  </p>]]></content>
    
    
    <categories>
      
      <category>science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ENGLISH</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>行人重识别深度学习方法</title>
    <link href="/2020/05/23/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <url>/2020/05/23/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/keenJMS/Person-re-identification/blob/master/%E7%BB%BC%E8%BF%B0/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95.pdf" target="_blank" rel="noopener">文献链接</a><br><strong><em>罗浩, 姜伟, 范星, 张思朋. 基于深度学习的行人重识别研究进展. 自动化学报, 2019,45 (11):2032-2049</em></strong>   </p><p>&emsp;&emsp;<strong>按照训练损失可以分为基于表征学习和度量学习, 根据特征是否考虑局部特征可以分为基于全局特征和基于局部特征, 根据数据不同可以分为基于单帧图像和基于视频序列的方法</strong>  </p><h1 id="基于表征学习的方法"><a href="#基于表征学习的方法" class="headerlink" title="基于表征学习的方法"></a>基于表征学习的方法</h1><p>&emsp;&emsp;基于表征学习 (Representation learning) 的方法是一类非常常用的行人重识别方法. 虽然行人重识别的最终目标是为了学习出两张图片之间的相似度, 但是表征学习的方法并没有直接在训练网络的时候考虑图片间的相似度, 而把行人重识别任务当做分类 (Classiflcation) 问题或者验证(Veriflcation) 问题来看待. 这类方法的特点就是网络的最后一层全连接 (Fully connected, FC) 层输出的并不是最终使用的图像特征向量, 而是经过一个Softmax 激活函数来计算表征学习损失, 前一层 (倒数第二层) FC 层通常为特征向量层. </p><p>&emsp;&emsp;<strong>分类</strong> 问题是指利用行人的 ID 或者属性等作为训练标签来训练模型, 每次只需要输入一张图片;分类网络常用的两种损失分别是行人 ID 损失(Identiflcation loss) 和属性损失 (Attribute loss).文献将每一个行人当做分类问题的一个类别, 用行人的 ID 作为训练数据的标签来训练 CNN网络, 这个网络损失被称为ID 损失, 而这种网络被称为 IDE (ID embedding) 网络,IDE是行人重识别领域非常重要的 baseline 基准.后来部分研究者认为, 光靠行人的 ID 信息不足以学习出一个泛化能力足够强的模型. 因此, 他们利用了额外标注的行人图片的属性信息, 例如性别、 头发、 衣着等属性, 通过引入行人属性标签计算属性损失. 训练好的网络不但要准确地预测出行人 ID, 还要预测出各项行人属性, 这大大增加了网络的泛化能力, 多数论文也显示这种方法是有效的.<br><img src="/img/ID%E6%8D%9F%E5%A4%B1%E5%92%8C%E5%B1%9E%E6%80%A7%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%AE%9E%E4%BE%8B.jpg" srcset="/img/loading.gif" alt=""><br>&emsp;&emsp;<strong>验证</strong> 问题是指输入一对 (两张) 行人图片, 让网络来学习这两张图片是否属于同一个行人.损失函数一般为行人ID损失和属性损失.验证网络每次需要输入两张图片, 这两张图片经过一个共享的 CNN 网络, 将网络输出的两个特征向量融合起来输入到一个只有两个神经元的 FC 层, 来预测这两幅图片是否属于同一个行人. 因此, 验证网络本质上是一个<strong>多输入单输出</strong>的二分类网络.通常,仅仅使用验证损失训练网络是非常低效的, 所以验证损失会与 ID 损失一起使用来训练网络.<br><img src="/img/%E7%BB%93%E5%90%88%E9%AA%8C%E8%AF%81%E6%8D%9F%E5%A4%B1%E5%92%8CID%E6%8D%9F%E5%A4%B1%E7%BD%91%E7%BB%9C%E7%A4%BA%E4%BE%8B.jpg" srcset="/img/loading.gif" alt="">  </p><h1 id="基于度量学习的方法"><a href="#基于度量学习的方法" class="headerlink" title="基于度量学习的方法"></a>基于度量学习的方法</h1><p>&emsp;&emsp;度量学习 (Metric learning) 是广泛用于图像检索领域的一种方法. 不同于表征学习, 度量学习旨在通过网络学习出两张图片的相似度. 在行人重识别问题上, 表现为同一行人的不同图片间的相似度大于不同行人的不同图片.定义一个映射，使图片从原始域映射到特征域，之后再定义一个距离度量函数，计算两个特征向量的距离，最后通过最小化网络的度量损失, 来寻找一个最优的映射.使得相同行人两张图片 (正样本对) 的距离尽可能小, 不同行人两张图片 (负样本对) 的距离尽可能大. 而这个映射 f(x), 就是我们训练得到的深度卷积网络.常用的损失方法有对比损失，三元组损失和四元组损失.<br>&emsp;&emsp;度量学习可以近似看作为样本在特征空间进行聚类, 表征学习可以近似看作为学习样本在特征空间的分界面. 正样本距离拉近的过程使得类内距离缩小, 负样本距离推开的过程使得类间距离增大, 最终收敛时样本在特征空间呈现聚类效应. 度量学习和表征学习相比, 优势在于网络末尾不需要接一个分类的全连接层, 因此对于训练集的行人 ID 数量并不敏感, 可以应用于训练超大规模数据集的网络. 总体而言, 度量学习比表征学习使用的更加广泛, 性能表现也略微优于表征学习. 但是目前行人重识别的数据集规模还依然有限, 表征学习的方法也依然得到使用, 而同时融合度量学习和表征学习训练网络的思路也在逐渐变得流行.</p><h1 id="基于局部特征的方法"><a href="#基于局部特征的方法" class="headerlink" title="基于局部特征的方法"></a>基于局部特征的方法</h1><p>&emsp;&emsp;上文介绍的两类方法是从损失函数的角度进行分类的，从图像特征的角度看，又可分为基于全局特征和基于局部特征的方法.全局特征比较简单,是指让网络对整幅图像提取一个特征,不考虑局部特征.局部特征是指手动或者自动地让网络去关注关键的局部区域, 然后提取这些区域的局部特征. 常用的提取局部特征的思路主要有图像切块、 利用骨架关键点定位以及行人前景分割等.<br><img src="/img/%E5%88%A9%E7%94%A8%E5%9B%BE%E7%89%87%E5%88%87%E5%9D%97%E6%8F%90%E5%8F%96%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81%E7%A4%BA%E4%BE%8B.jpg" srcset="/img/loading.gif" alt=""><br><img src="/img/%E5%88%A9%E7%94%A8%E5%A7%BF%E6%80%81%E7%82%B9%E6%8F%90%E5%8F%96%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81%E7%A4%BA%E4%BE%8B.jpg" srcset="/img/loading.gif" alt="">  </p><h1 id="基于视频序列的方法"><a href="#基于视频序列的方法" class="headerlink" title="基于视频序列的方法"></a>基于视频序列的方法</h1><p>&emsp;&emsp;这类方法考虑了图像的内容信息, 还会考虑: 1) 帧与帧之间的运动信息; 2) 更好的特征融合; 3) 对图像帧进行质量判断等. 总体来说, 基于序列的方法核心思想为通过融合更多的信息来解决图像噪声较大、 背景复杂等一系列质量不佳的问题.一种思路是融合图像内容信息和运动信息，如下图所示；另一种思路是对图像帧进行质量判断,保留更多的高质量图像的特征.<br><img src="/img/%E8%9E%8D%E5%90%88%E5%86%85%E5%AE%B9%E4%BF%A1%E6%81%AF%E5%92%8C%E8%BF%90%E5%8A%A8%E4%BF%A1%E6%81%AF%E7%9A%84AMOC%E7%BD%91%E7%BB%9C.jpg" srcset="/img/loading.gif" alt="">  </p>]]></content>
    
    
    <categories>
      
      <category>science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ReID</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Challenge Of ReID</title>
    <link href="/2020/05/23/The-Challenge-Of-ReID/"/>
    <url>/2020/05/23/The-Challenge-Of-ReID/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/keenJMS/Person-re-identification/blob/master/%E7%BB%BC%E8%BF%B0/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95.pdf" target="_blank" rel="noopener">文献链接</a><br>(1)跨视角造成的姿态多变问题: 由于不同摄像头架设的角度、 位置不一, 拍摄图片中的行人姿态也十分多变. 目前已经有不少代表性的工作从不同角度上来解决这个问题, 而这些方法主要是依靠一个预训练的姿态模型来实现姿态的对齐.<br>(2)行人图片分辨率变化: 由于摄像头中目标拍摄距离不一致, 拍摄的行人图片分辨率也不一样.这方面方法比较少，已知的有sing方法。<strong>SING</strong>先用高分辨率图片降采样得到一批低分辨率图片. 之后, 网络优化联合学习图像超分辨的重构损失和行人身份识别损失函数. 低分辨率图片经过网络高分辨率处理后再进行特征提取, 而正常分辨率图像则是直接进行特征提取. 由于不同分辨率的图片经过不同的方式提取特征, 因此 SING 网络能够较好地应对分辨率变化的问题.<br>(3)行人图片遮挡问题: 目前学术界的行人重识别数据集大多数清洗过的高质量图像. 然而在真实的使用场景, 行人经常会被移动目标或者静态物体遮挡, 造成行人图片的不完整. 由于失去了部分行人特征而引入了很多干扰特征, 使得很多基于全局特征的行人重识别算法效果大大下降. 一个思路是利用行人姿态模型来估计行人图像的可视部分, 然后对可视部分进行局部特征提取、 融合.<br>(4)图像域变化的跨模态重识别. 图像域的变化是行人重识别应用上非常普遍的一个挑战. 图像域变化的类型也多种多样, 例如不同相机、 不同天气、不同时间、 不同城市拍摄的图像风格均可能不同. 此外, 夜晚 RGB 相机也会失效, 使用红外相机拍摄的图片没有颜色信息, 因此 RGB 图片与红外图片的行人重识别也是个典型的跨模态问题. 目前基于 GAN网络生成图像来解决图像域偏差是一个很流行的思路。</p>]]></content>
    
    
    <categories>
      
      <category>science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ReID</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>行人再识别技术综述</title>
    <link href="/2020/05/22/%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/"/>
    <url>/2020/05/22/%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/keenJMS/Person-re-identification/blob/master/%E7%BB%BC%E8%BF%B0/%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0.pdf" target="_blank" rel="noopener">文献链接</a></p><h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h1><h2 id="1-1什么是行人重识别"><a href="#1-1什么是行人重识别" class="headerlink" title="1.1什么是行人重识别"></a>1.1什么是行人重识别</h2><p>行人再识别 (Person re-identiflcation, Re-ID)起源于多摄像头跟踪, 用于判断非重叠视域中拍摄到的不同图像中的行人是否属于同一个人.广泛应用于智能视频监控、安保等领域.行人图像的分辨率变化大、拍摄角度不统一、光照条件差、环境变化大、行人姿态不断变化等原因,使得行人再识别成为目前计算机视觉领域一个既具有研究价值又极具挑战性的研究热点和难点问题.<br>行人再识别典型流程如下，对于摄像头 A 和 B 采集的图像/视频, 首先进行行人检测, 得到行人图像.为了消除行人检测效果对再识别结果的影响.大部分行人再识别算法使用已经裁剪好的行人图像作为输入.然后针对输入图像中提取稳定、鲁棒的特征,得能够描述和区分不同行人的特征表达向量.最后根据特征表达向量进行相似性度量, 按照相似性大小对图像进行排序, 相似度最高的图像将作为最终的识别结果.<br><img src="/img/%E7%BB%BC%E8%BF%B01.jpg" srcset="/img/loading.gif" alt="综述1"><br>特征提取与表达.从行人外观出发,提取鲁棒性强且具有较强区分性的特征表示向量,有效表达行人图像的特性; 2)相似性度量. 通过特征向量之间的相似度比对, 判断行人的相似性.可以看出, 行人再识别与图像检索的思路相同, 可以看作是图像检索的子问题.</p><h1 id="2-基于人工设计特征的行人再识别"><a href="#2-基于人工设计特征的行人再识别" class="headerlink" title="2.基于人工设计特征的行人再识别"></a>2.基于人工设计特征的行人再识别</h1><h2 id="2-1特征提取与表达"><a href="#2-1特征提取与表达" class="headerlink" title="2.1特征提取与表达"></a>2.1特征提取与表达</h2><h3 id="2-1-1低层视觉特征"><a href="#2-1-1低层视觉特征" class="headerlink" title="2.1.1低层视觉特征"></a>2.1.1低层视觉特征</h3><p>颜色特征：颜色直方图、颜色矩、颜色相关图、颜色聚合向量.<br>纹理特征：纹理特征涉及到相邻像素的比较, 对光照具有鲁棒性.<br>图像分割方法：行人再识别方法在颜色和纹理特征中加入空间区域信息.行人图像被分成多个重叠或非重叠的局部图像块, 然后分别从中提取颜色或纹理特征, 从而为行人特征增加空间区域信息.<br><img src="/img/%E7%BB%BC%E8%BF%B02.jpg" srcset="/img/loading.gif" alt="综述2"><br><img src="/img/%E7%BB%BC%E8%BF%B03.jpg" srcset="/img/loading.gif" alt="综述3"><br>低层特征的提取不需要复杂的训练过程, 可解释性较强. 但是表达能力较弱, 面对复杂的识别环境其泛化能力受到一定制约, 无法针对具体的行人再识别任务进行优化.  </p><h3 id="2-1-2中层滤波器特征"><a href="#2-1-2中层滤波器特征" class="headerlink" title="2.1.2中层滤波器特征"></a>2.1.2中层滤波器特征</h3><p>中层滤波器特征是利用聚类算法, 从行人图像中学习出一系列有表达能力的滤波器. 每一个滤波器都代表一种与身体特定部位相关的视觉模式,也称显著区域 (Salient region).<br><img src="/img/%E7%BB%BC%E8%BF%B04.jpg" srcset="/img/loading.gif" alt="综述4"><br>人体由各个身体部位组成, 具有良好的结构特性, 使用与人体部位对应的滤波器特征能够平衡行人描述符的区分能力和泛化能力. 低层和中层特征结合起来使用能够充分发挥各自的优势, 在一定程度上克服行人再识别中的光照和视角变化问题. 但是, 人体是非刚性目标, 外观易受到姿态、 遮挡等各种因素的影响, 仅利用低层和中层特征会导致识别精度不高, 还需要利用其他更高层的特征.  </p><h3 id="2-1-3高层属性特征"><a href="#2-1-3高层属性特征" class="headerlink" title="2.1.3高层属性特征"></a>2.1.3高层属性特征</h3><p>人类在辨识行人时会使用离散而精确的特有属性 (Attribute)，例如服装样式、 性别、 胖瘦等都属于行人的属性特征. 行人图像对应的属性特征通常采用离散的二进制向量表示形式.与其他特征相比, 高层属性特征尽管在提取和表达方面复杂,属性标定需要大量的人工和时间成本, 但含有更加丰富的语义信息, 而且对于光照和视角变化具有更强的鲁棒性. 因此, 属性特征与低层特征联合使用,可以有效提高识别性能.</p><h3 id="2-1-4视频时空特征"><a href="#2-1-4视频时空特征" class="headerlink" title="2.1.4视频时空特征"></a>2.1.4视频时空特征</h3><p>在基于视频的行人再识别中, 每个行人至少包含两段跨视域的视频序列, 其中包含数量不等的视频帧. 这些视频帧能够提供大量的训练样本, 可以更方便地训练机器学习算法, 从而提高识别的性能.<br>最常用的方法是提取每一帧的低层特征, 然后利用平均/最大池化方法将其聚合为一个全局特征向量, 用以反映行人的外观信息.<br>时空特征反映了视频中的运动信息,是行人外观特征的有效补充. 然而, 时空特征易受视角、 尺度和速度等因素的影响, 在新型的大型行人再识别数据集上表现得差强人意.</p><h2 id="2-2相似性度量"><a href="#2-2相似性度量" class="headerlink" title="2.2相似性度量"></a>2.2相似性度量</h2><p>  根据提取出的特征之间的相似性，判断行人图像是否为同一个人.选择合适的相似性度量方法对行人再识别<br>至关重要. 根据度量过程中是否使用标签, 相似性度量可以分为无监督度量和监督度量.</p><h3 id="2-2-1无监督度量"><a href="#2-2-1无监督度量" class="headerlink" title="2.2.1无监督度量"></a>2.2.1无监督度量</h3><p>无监督度量直接利用特征表达阶段获得的特征向量进行相似性度量. 特征向量之间的相似性往往通过特征向量之间的距离进行度量, 特征向量之间的距离越小, 说明行人图像越相似.比如<strong>欧式距离，巴氏距离</strong>等，这类的缺点是将特征向量的每个维度都等同看待，但实际中每个维度的重要性是不同的.</p><h3 id="2-2-2监督度量"><a href="#2-2-2监督度量" class="headerlink" title="2.2.2监督度量"></a>2.2.2监督度量</h3><p>距离度量学习是基于成对约束的监督度量方法,基本思路是利用给定的训练样本集学习得到一个能够有效反映数据样本间相似度的度量矩阵, 在减少同类样本之间距离的同时, 增大非同类样本之间的距离. 当特征向量提供的信息足够充足时, 距离度量能够获得比非监督方式更高的区分能力. 但是, 与非监督度量方法相比, 距离度量学习需要额外的学习过程, 在训练样本不足时容易产生过拟合现象, 且图像库和场景变化时需要重新训练.<br>常用<strong>马氏距离</strong>.</p><h1 id="3-基于深度学习的行人再识别"><a href="#3-基于深度学习的行人再识别" class="headerlink" title="3.基于深度学习的行人再识别"></a>3.基于深度学习的行人再识别</h1><p>深度学习与传统方法的最大不同在于其特征是从大数据中自动学习得到的, 通过建立类似于人脑的分层模型结构, 能够从大量数据中逐级提取由底层到高层的特征, 获得适合于分类或者识别的深度特征.<br>一个涌入ReID的深度神经网络一般包含卷积层(提取图像的各种信息, 例如边缘和形状)、池化层(对卷积后的特征信号进行抽象, 从而大幅减少训练参数, 另外还可以减少过拟合现象的出现)和全连接层(将池化层得到的特征图投影到一维的特征空间, 形成行人图像的特征向量)，如下图所示：<br><img src="/img/%E7%BB%BC%E8%BF%B05.jpg" srcset="/img/loading.gif" alt="综述5"><br>首先将不同视域中的行人图像作为网络的输入, 然后将这些图像分解为不同的颜色通道子图分别进行处理. 对于每幅子图, 在接下来的卷积层中对其实施卷积滤波操作, 得到不同局部图像块的响应, 作为局部特征. 这些局部特征组合起来, 形成特征图, 作为该卷积层的输出.<br>深度学习模型可以将特征表达和相似度量两个环节整合在一起，按整合方式的不同可分为<strong>端到端、混合式和独立式</strong>.<br><img src="/img/%E7%BB%BC%E8%BF%B06.jpg" srcset="/img/loading.gif" alt="综述6"></p><h2 id="3-1端到端式的深度行人再识别"><a href="#3-1端到端式的深度行人再识别" class="headerlink" title="3.1端到端式的深度行人再识别"></a>3.1端到端式的深度行人再识别</h2><p>端到端式的行人再识别利用深度学习模型, 将特征提取和相似性度量这两个主要环节整合到一个统一的框架下进行联合优化, 形成一种端到端的行人再识别方案.<strong>Siamese网络，递归神经网络RNN,长短期记忆网络LSTM</strong><br>早期的端到端方法在进行相似性比较时, 往往采用简单的欧氏距离或余弦距离, 缺少距离学习的过程, 影响了识别准确率. 常见的解决方法是在深度网络训练过程中加入损失函数约束, 使得同类样本距离变小, 异类样本距离变大, 达到距离学习的效果.</p><h2 id="3-2混合式的深度行人再识别"><a href="#3-2混合式的深度行人再识别" class="headerlink" title="3.2混合式的深度行人再识别"></a>3.2混合式的深度行人再识别</h2><p>该方法可以采用较为成熟的人工特征表达行人的局部特性, 采用浅层的网络结构提取行人的全局特征, 二者结合可以充分发挥各自优势, 在一定程度上弥补训练数据的不足, 同时可以在一定程度上避免深度网络模型过于复杂、 网络训练速度慢的缺点.  </p><h2 id="3-3独立式的深度行人再识别"><a href="#3-3独立式的深度行人再识别" class="headerlink" title="3.3独立式的深度行人再识别"></a>3.3独立式的深度行人再识别</h2><p>独立式的深度行人再识别方法的框架与基于人工特征的方法相似, 不同的是采用深度神经网络提取行人图像的深度特征, 然后结合距离度量学习方法完成行人再识别.基于深度学习的行人再识别方法仅经过极少的预处理就可以得到从原始像素到高层语义的有效特征表达. 另外, 在行人图像中, 各种复杂的因素, 包括姿态、 性别、 着装等, 往往以非线性的方式组合在一起, 而深度学习可以通过多层非线性映射将这些因素分开, 利用不同的神经元代表不同因素, 使其变成简单的线性关系, 不再相互影响, 从而提升识别效果.</p><h1 id="4-数据集"><a href="#4-数据集" class="headerlink" title="4.数据集"></a>4.数据集</h1><p><img src="/img/%E6%95%B0%E6%8D%AE%E9%9B%86.jpg" srcset="/img/loading.gif" alt="数据集"><br>VIPeR、 CUHK01 和 Market-1501 均为基于图像的行人再识别数据集.PRID-2011、 iLIDS-VID 和 MARS 均为基于视频的行人再识别数据集.<br>累计匹配性能曲线 (Cumulative match characteristic, CMC) 和 Rank-N 表格.  常见的有 Rank-1, Rank-5,Rank-10 和 Rank-20. 其中 Rank-5 代表在前 5 幅图像中可以正确匹配的概率, 概率值越大表示效果<br>越好.<br><img src="/img/RANKN.jpg" srcset="/img/loading.gif" alt="RANKN">    </p><h1 id="5-展望"><a href="#5-展望" class="headerlink" title="5.展望"></a>5.展望</h1><p>未来的发展方向有：长时间的行人再识别，结合多模态生物线索的行人再识别.密集场景与低分辨率环境下的行人再识别.设计鲁棒的语义级行人特征表达.基于深度属性的行人再识别. </p>]]></content>
    
    
    <categories>
      
      <category>science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ReID</tag>
      
      <tag>Summary</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/05/21/hello-world/"/>
    <url>/2020/05/21/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="This-is-my-blog"><a href="#This-is-my-blog" class="headerlink" title="This is my blog"></a>This is my blog</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="hljs bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="hljs bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="hljs bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
